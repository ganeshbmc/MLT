{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Class  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall start writing the `Network Class`. The two methods that are indispensible for any ML class are:  \n",
    "\n",
    "* `fit`  \n",
    "* `predict`  \n",
    "* \n",
    "Fitting a neural network model requires us to compute two passes on the data:  \n",
    "\n",
    "* `forward`  \n",
    "* `backward`  \n",
    "\n",
    "We need to start at some place by initializing the network and various hyperparameters and this requires an `init` method:  \n",
    "\n",
    "* `init`  \n",
    "\n",
    "In most of these methods, we would have to take help of certain `helper` functions:  \n",
    "\n",
    "* `activations`  \n",
    "* `losses`  \n",
    "\n",
    "This is the process but we will work through it in reverse order so that each step of the process does not have any forward references:  \n",
    "\n",
    "* `helpers` --> `init` --> `forward` --> `backward` --> `fit` --> `predict`  \n",
    "\n",
    "The skeleton of the class is given in the code block below. For ease of exposition, we are going to discuss the methods one at a time and then plug them into the class right at the end.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skeleton of Network Class  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "\n",
    "    def init(self, layers, activation_choice = 'relu',\n",
    "                        output_choice = 'softmax',\n",
    "                        loss_choice = 'cce'):\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        pass\n",
    "\n",
    "    def backward(self, Y, Y_hat):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, lr = 0.01,\n",
    "                    epochs = 100,\n",
    "                    batch_size = 100):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden layer  \n",
    "\n",
    "We will look at two activation functions for the hidden layers. Both these functions will be applied element-wise. The input to these functions can be scalars, vectors or matrices.  \n",
    "\n",
    "* `Sigmoid`  \n",
    "* `ReLU`  \n",
    "\n",
    "We also need the derivatives of these functions while computing the backward pass. Deriving the mathematical expressions for them are left as an exercise to the learners.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def grad_sigmoid(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.where(z >= 0, z, 0)\n",
    "\n",
    "def grad_relu(z):\n",
    "    return np.where(z >= 0, 1, 0)\n",
    "\n",
    "# A dictionary of activation functions will be used while initializing the network  \n",
    "hidden_act = {'sigmoid': sigmoid, 'relu': relu}\n",
    "grad_hidden_act = {'sigmoid': grad_sigmoid, 'relu': grad_relu}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output layer  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the two activation functions for the output layer.  \n",
    "\n",
    "* `Identity` for regression  \n",
    "* `Softmax` for classification  \n",
    "\n",
    "**Note**: In softmax, to avoid overflow, we will subtract the row-wise maximum from each row while computing the softmax.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(z):\n",
    "    return z\n",
    "\n",
    "def softmax(z):\n",
    "    '''Row-wise softmax'''\n",
    "    # Check if z is a matrix\n",
    "    assert z.ndim == 2\n",
    "    \n",
    "    # To prevent overflow, subtract softmax row-wise\n",
    "    z -= z.max(axis=1, keepdims=True)\n",
    "\n",
    "    # Compute row-wise softmax\n",
    "    prob = np.exp(z) / np.exp(z).sum(axis=1, keep_dims=True)\n",
    "\n",
    "    # Check if the row probability is a probability distribution\n",
    "    assert np.allclose(prob.sum(axis=1), np.ones(z.shape[0]))\n",
    "    return prob\n",
    "\n",
    "output_act = {'softmax': softmax, 'identity': identity}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of losses we will use:  \n",
    "\n",
    "* `Least square error` for regression  \n",
    "* `Categorical cross-entropy` loss for classification  \n",
    "\n",
    "[video link](https://youtu.be/q1GTG13OgNY?t=345)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square(y, y_hat):\n",
    "    return 0.5 * np.sum((y_hat - y) * (y_hat - y))\n",
    "\n",
    "def cce(Y, Y_hat):      # Note that capital Y is used to denote that Y is a matrix with shape n*k\n",
    "    return -np.sum(Y * np.log(Y_hat))\n",
    "\n",
    "losses = {'least_square': least_square, 'cce': cce}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will look at two parts:  \n",
    "\n",
    "* Network architecture  \n",
    "* Weight initialization  \n",
    "\n",
    "`Network architecture`  \n",
    "\n",
    "The following components mainly determine the structure of the network:  \n",
    "* number of layers  \n",
    "* number of neurons per layer  \n",
    "\n",
    "We will use `l` to index the layers. The network has `L` layers in all.  \n",
    "\n",
    "* `l = 0`: Input layer  \n",
    "* `1 <= l <= L -1`: Hidden layers  \n",
    "* `l = L`: Output layer  \n",
    "\n",
    "We shall represent the number of layers and the neurons using a list `layers`. The variable L will never make an explicit appearance anywhere, instead we will use `range(len(layers))` to iterate through the layers.  \n",
    "\n",
    "One useful task is to compute the total number of parameters in the network. This will come in handy later on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(layers):\n",
    "    num_params = 0\n",
    "    for l in range(1, len(layers)):\n",
    "        num_weights = layers[l-1] * layers[l]\n",
    "        num_biases = layers[l]\n",
    "        num_params += (num_weights + num_biases)\n",
    "    return num_params\n",
    "\n",
    "# Test count_params  \n",
    "assert count_params([64, 5, 10]) == (64 * 5 + 5) + (5 * 10 + 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Parameter initialization`  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78ddfc3686b8b7161f2836984651df038ec9a0366954334fc42499f59ad2b3c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
